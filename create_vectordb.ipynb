{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9564460",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T20:01:30.376360Z",
     "start_time": "2025-12-03T20:01:30.360491Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c32b8ad3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T20:01:41.162434Z",
     "start_time": "2025-12-03T20:01:30.522393Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: libmagic in c:\\users\\hp\\anaconda3\\envs\\py3.10\\lib\\site-packages (1.0)\n",
      "Requirement already satisfied: unstructured in c:\\users\\hp\\anaconda3\\envs\\py3.10\\lib\\site-packages (0.18.20)\n",
      "Requirement already satisfied: charset-normalizer in c:\\users\\hp\\anaconda3\\envs\\py3.10\\lib\\site-packages (from unstructured) (3.4.1)\n",
      "Requirement already satisfied: filetype in c:\\users\\hp\\anaconda3\\envs\\py3.10\\lib\\site-packages (from unstructured) (1.2.0)\n",
      "Requirement already satisfied: python-magic in c:\\users\\hp\\anaconda3\\envs\\py3.10\\lib\\site-packages (from unstructured) (0.4.27)\n",
      "Requirement already satisfied: lxml in c:\\users\\hp\\anaconda3\\envs\\py3.10\\lib\\site-packages (from unstructured) (5.4.0)\n",
      "Requirement already satisfied: nltk in c:\\users\\hp\\anaconda3\\envs\\py3.10\\lib\\site-packages (from unstructured) (3.9.1)\n",
      "Requirement already satisfied: requests in c:\\users\\hp\\anaconda3\\envs\\py3.10\\lib\\site-packages (from unstructured) (2.32.3)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\hp\\anaconda3\\envs\\py3.10\\lib\\site-packages (from unstructured) (4.13.3)\n",
      "Requirement already satisfied: emoji in c:\\users\\hp\\anaconda3\\envs\\py3.10\\lib\\site-packages (from unstructured) (2.15.0)\n",
      "Requirement already satisfied: dataclasses-json in c:\\users\\hp\\anaconda3\\envs\\py3.10\\lib\\site-packages (from unstructured) (0.6.7)\n",
      "Requirement already satisfied: python-iso639 in c:\\users\\hp\\anaconda3\\envs\\py3.10\\lib\\site-packages (from unstructured) (2025.11.11)\n",
      "Requirement already satisfied: langdetect in c:\\users\\hp\\anaconda3\\envs\\py3.10\\lib\\site-packages (from unstructured) (1.0.9)\n",
      "Requirement already satisfied: numpy in c:\\users\\hp\\anaconda3\\envs\\py3.10\\lib\\site-packages (from unstructured) (2.2.6)\n",
      "Requirement already satisfied: rapidfuzz in c:\\users\\hp\\anaconda3\\envs\\py3.10\\lib\\site-packages (from unstructured) (3.14.3)\n",
      "Requirement already satisfied: backoff in c:\\users\\hp\\anaconda3\\envs\\py3.10\\lib\\site-packages (from unstructured) (2.2.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\hp\\anaconda3\\envs\\py3.10\\lib\\site-packages (from unstructured) (4.15.0)\n",
      "Requirement already satisfied: unstructured-client in c:\\users\\hp\\anaconda3\\envs\\py3.10\\lib\\site-packages (from unstructured) (0.42.4)\n",
      "Requirement already satisfied: wrapt in c:\\users\\hp\\anaconda3\\envs\\py3.10\\lib\\site-packages (from unstructured) (1.17.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hp\\anaconda3\\envs\\py3.10\\lib\\site-packages (from unstructured) (4.67.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\hp\\anaconda3\\envs\\py3.10\\lib\\site-packages (from unstructured) (5.9.0)\n",
      "Requirement already satisfied: python-oxmsg in c:\\users\\hp\\anaconda3\\envs\\py3.10\\lib\\site-packages (from unstructured) (0.0.2)\n",
      "Requirement already satisfied: html5lib in c:\\users\\hp\\anaconda3\\envs\\py3.10\\lib\\site-packages (from unstructured) (1.1)\n",
      "Requirement already satisfied: markdown in c:\\users\\hp\\anaconda3\\envs\\py3.10\\lib\\site-packages (from unstructured[md]) (3.10)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\hp\\anaconda3\\envs\\py3.10\\lib\\site-packages (from beautifulsoup4->unstructured) (2.6)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\hp\\anaconda3\\envs\\py3.10\\lib\\site-packages (from dataclasses-json->unstructured) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\hp\\anaconda3\\envs\\py3.10\\lib\\site-packages (from dataclasses-json->unstructured) (0.9.0)\n",
      "Requirement already satisfied: six>=1.9 in c:\\users\\hp\\anaconda3\\envs\\py3.10\\lib\\site-packages (from html5lib->unstructured) (1.17.0)\n",
      "Requirement already satisfied: webencodings in c:\\users\\hp\\anaconda3\\envs\\py3.10\\lib\\site-packages (from html5lib->unstructured) (0.5.1)\n",
      "Requirement already satisfied: click in c:\\users\\hp\\anaconda3\\envs\\py3.10\\lib\\site-packages (from nltk->unstructured) (8.3.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\hp\\anaconda3\\envs\\py3.10\\lib\\site-packages (from nltk->unstructured) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\hp\\anaconda3\\envs\\py3.10\\lib\\site-packages (from nltk->unstructured) (2024.11.6)\n",
      "Requirement already satisfied: olefile in c:\\users\\hp\\anaconda3\\envs\\py3.10\\lib\\site-packages (from python-oxmsg->unstructured) (0.47)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\anaconda3\\envs\\py3.10\\lib\\site-packages (from requests->unstructured) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\anaconda3\\envs\\py3.10\\lib\\site-packages (from requests->unstructured) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\anaconda3\\envs\\py3.10\\lib\\site-packages (from requests->unstructured) (2025.1.31)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\anaconda3\\envs\\py3.10\\lib\\site-packages (from tqdm->unstructured) (0.4.6)\n",
      "Requirement already satisfied: aiofiles>=24.1.0 in c:\\users\\hp\\anaconda3\\envs\\py3.10\\lib\\site-packages (from unstructured-client->unstructured) (24.1.0)\n",
      "Requirement already satisfied: cryptography>=3.1 in c:\\users\\hp\\anaconda3\\envs\\py3.10\\lib\\site-packages (from unstructured-client->unstructured) (44.0.2)\n",
      "Requirement already satisfied: httpcore>=1.0.9 in c:\\users\\hp\\anaconda3\\envs\\py3.10\\lib\\site-packages (from unstructured-client->unstructured) (1.0.9)\n",
      "Requirement already satisfied: httpx>=0.27.0 in c:\\users\\hp\\anaconda3\\envs\\py3.10\\lib\\site-packages (from unstructured-client->unstructured) (0.27.2)\n",
      "Requirement already satisfied: pydantic>=2.11.2 in c:\\users\\hp\\anaconda3\\envs\\py3.10\\lib\\site-packages (from unstructured-client->unstructured) (2.12.4)\n",
      "Requirement already satisfied: pypdf>=6.2.0 in c:\\users\\hp\\anaconda3\\envs\\py3.10\\lib\\site-packages (from unstructured-client->unstructured) (6.2.0)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\hp\\anaconda3\\envs\\py3.10\\lib\\site-packages (from unstructured-client->unstructured) (1.0.0)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\hp\\anaconda3\\envs\\py3.10\\lib\\site-packages (from cryptography>=3.1->unstructured-client->unstructured) (1.17.1)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\hp\\anaconda3\\envs\\py3.10\\lib\\site-packages (from httpcore>=1.0.9->unstructured-client->unstructured) (0.16.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\hp\\anaconda3\\envs\\py3.10\\lib\\site-packages (from httpx>=0.27.0->unstructured-client->unstructured) (4.9.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\hp\\anaconda3\\envs\\py3.10\\lib\\site-packages (from httpx>=0.27.0->unstructured-client->unstructured) (1.3.1)\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\users\\hp\\anaconda3\\envs\\py3.10\\lib\\site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->unstructured) (24.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\hp\\anaconda3\\envs\\py3.10\\lib\\site-packages (from pydantic>=2.11.2->unstructured-client->unstructured) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in c:\\users\\hp\\anaconda3\\envs\\py3.10\\lib\\site-packages (from pydantic>=2.11.2->unstructured-client->unstructured) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\hp\\anaconda3\\envs\\py3.10\\lib\\site-packages (from pydantic>=2.11.2->unstructured-client->unstructured) (0.4.2)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\hp\\anaconda3\\envs\\py3.10\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json->unstructured) (1.0.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\hp\\anaconda3\\envs\\py3.10\\lib\\site-packages (from cffi>=1.12->cryptography>=3.1->unstructured-client->unstructured) (2.22)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\hp\\anaconda3\\envs\\py3.10\\lib\\site-packages (from anyio->httpx>=0.27.0->unstructured-client->unstructured) (1.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "%pip install libmagic unstructured unstructured[md]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce28abd",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-12-03T20:06:43.100087Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 12427 previously processed files from log.\n",
      "\n",
      "üì¶ Loading existing FAISS index from: vectorstores\\index\n",
      "\n",
      "üîç Walking root: data/processed_data\n",
      "‚úÖ Folder already fully processed, skipping: data/processed_data\\AAPL\n",
      "‚úÖ Folder already fully processed, skipping: data/processed_data\\ABBV\n",
      "‚úÖ Folder already fully processed, skipping: data/processed_data\\ABT\n",
      "‚úÖ Folder already fully processed, skipping: data/processed_data\\ACN\n",
      "‚úÖ Folder already fully processed, skipping: data/processed_data\\ADBE\n",
      "‚úÖ Folder already fully processed, skipping: data/processed_data\\AIG\n",
      "‚úÖ Folder already fully processed, skipping: data/processed_data\\AMD\n",
      "‚úÖ Folder already fully processed, skipping: data/processed_data\\AMGN\n",
      "‚úÖ Folder already fully processed, skipping: data/processed_data\\AMT\n",
      "‚úÖ Folder already fully processed, skipping: data/processed_data\\AMZN\n",
      "‚úÖ Folder already fully processed, skipping: data/processed_data\\AVGO\n",
      "‚úÖ Folder already fully processed, skipping: data/processed_data\\AXP\n",
      "‚úÖ Folder already fully processed, skipping: data/processed_data\\BA\n",
      "‚úÖ Folder already fully processed, skipping: data/processed_data\\BK\n",
      "‚úÖ Folder already fully processed, skipping: data/processed_data\\BKNG\n",
      "‚úÖ Folder already fully processed, skipping: data/processed_data\\BLK\n",
      "‚úÖ Folder already fully processed, skipping: data/processed_data\\BMY\n",
      "‚úÖ Folder already fully processed, skipping: data/processed_data\\BRK-B\n",
      "‚úÖ Folder already fully processed, skipping: data/processed_data\\CAT\n",
      "‚úÖ Folder already fully processed, skipping: data/processed_data\\CHTR\n",
      "‚úÖ Folder already fully processed, skipping: data/processed_data\\CL\n",
      "‚úÖ Folder already fully processed, skipping: data/processed_data\\CMCSA\n",
      "‚úÖ Folder already fully processed, skipping: data/processed_data\\COF\n",
      "‚úÖ Folder already fully processed, skipping: data/processed_data\\company_news\n",
      "‚úÖ Folder already fully processed, skipping: data/processed_data\\COP\n",
      "‚úÖ Folder already fully processed, skipping: data/processed_data\\COST\n",
      "‚úÖ Folder already fully processed, skipping: data/processed_data\\CRM\n",
      "‚úÖ Folder already fully processed, skipping: data/processed_data\\CSCO\n",
      "‚úÖ Folder already fully processed, skipping: data/processed_data\\CVS\n",
      "‚úÖ Folder already fully processed, skipping: data/processed_data\\CVX\n",
      "‚úÖ Folder already fully processed, skipping: data/processed_data\\DE\n",
      "‚úÖ Folder already fully processed, skipping: data/processed_data\\DHR\n",
      "‚úÖ Folder already fully processed, skipping: data/processed_data\\DIS\n",
      "‚úÖ Folder already fully processed, skipping: data/processed_data\\DUK\n",
      "‚úÖ Folder already fully processed, skipping: data/processed_data\\EMR\n",
      "‚úÖ Folder already fully processed, skipping: data/processed_data\\FDX\n",
      "‚úÖ Folder already fully processed, skipping: data/processed_data\\GD\n",
      "‚úÖ Folder already fully processed, skipping: data/processed_data\\GE\n",
      "‚úÖ Folder already fully processed, skipping: data/processed_data\\GILD\n",
      "‚úÖ Folder already fully processed, skipping: data/processed_data\\GM\n",
      "‚úÖ Folder already fully processed, skipping: data/processed_data\\GOOGL\n",
      "‚úÖ Folder already fully processed, skipping: data/processed_data\\HD\n",
      "‚úÖ Folder already fully processed, skipping: data/processed_data\\HON\n",
      "‚úÖ Folder already fully processed, skipping: data/processed_data\\IBM\n",
      "‚úÖ Folder already fully processed, skipping: data/processed_data\\INTC\n",
      "‚úÖ Folder already fully processed, skipping: data/processed_data\\INTU\n",
      "‚úÖ Folder already fully processed, skipping: data/processed_data\\ISRG\n",
      "‚úÖ Folder already fully processed, skipping: data/processed_data\\JNJ\n",
      "‚úÖ Folder already fully processed, skipping: data/processed_data\\KO\n",
      "‚úÖ Folder already fully processed, skipping: data/processed_data\\LIN\n",
      "‚úÖ Folder already fully processed, skipping: data/processed_data\\LLY\n",
      "‚úÖ Folder already fully processed, skipping: data/processed_data\\LMT\n",
      "‚úÖ Folder already fully processed, skipping: data/processed_data\\LOW\n",
      "‚úÖ Folder already fully processed, skipping: data/processed_data\\MA\n",
      "‚úÖ Folder already fully processed, skipping: data/processed_data\\MCD\n",
      "‚úÖ Folder already fully processed, skipping: data/processed_data\\MDLZ\n",
      "‚úÖ Folder already fully processed, skipping: data/processed_data\\MDT\n",
      "‚úÖ Folder already fully processed, skipping: data/processed_data\\MET\n",
      "‚úÖ Folder already fully processed, skipping: data/processed_data\\META\n",
      "‚úÖ Folder already fully processed, skipping: data/processed_data\\MMM\n",
      "‚úÖ Folder already fully processed, skipping: data/processed_data\\MO\n",
      "‚úÖ Folder already fully processed, skipping: data/processed_data\\MRK\n",
      "‚úÖ Folder already fully processed, skipping: data/processed_data\\MSFT\n",
      "‚úÖ Folder already fully processed, skipping: data/processed_data\\NEE\n",
      "‚úÖ Folder already fully processed, skipping: data/processed_data\\NFLX\n",
      "‚úÖ Folder already fully processed, skipping: data/processed_data\\NKE\n",
      "‚úÖ Folder already fully processed, skipping: data/processed_data\\NOW\n",
      "‚úÖ Folder already fully processed, skipping: data/processed_data\\NVDA\n",
      "‚úÖ Folder already fully processed, skipping: data/processed_data\\ORCL\n",
      "‚úÖ Folder already fully processed, skipping: data/processed_data\\PEP\n",
      "‚úÖ Folder already fully processed, skipping: data/processed_data\\PFE\n",
      "‚úÖ Folder already fully processed, skipping: data/processed_data\\PG\n",
      "‚úÖ Folder already fully processed, skipping: data/processed_data\\PLTR\n",
      "‚úÖ Folder already fully processed, skipping: data/processed_data\\PM\n",
      "‚úÖ Folder already fully processed, skipping: data/processed_data\\PYPL\n",
      "‚úÖ Folder already fully processed, skipping: data/processed_data\\QCOM\n",
      "‚úÖ Folder already fully processed, skipping: data/processed_data\\RTX\n",
      "‚úÖ Folder already fully processed, skipping: data/processed_data\\SBUX\n",
      "‚úÖ Folder already fully processed, skipping: data/processed_data\\SCHW\n",
      "‚úÖ Folder already fully processed, skipping: data/processed_data\\SO\n",
      "‚úÖ Folder already fully processed, skipping: data/processed_data\\SPG\n",
      "‚úÖ Folder already fully processed, skipping: data/processed_data\\T\n",
      "‚úÖ Folder already fully processed, skipping: data/processed_data\\TGT\n",
      "‚úÖ Folder already fully processed, skipping: data/processed_data\\TMO\n",
      "‚úÖ Folder already fully processed, skipping: data/processed_data\\TMUS\n",
      "‚úÖ Folder already fully processed, skipping: data/processed_data\\TSLA\n",
      "‚úÖ Folder already fully processed, skipping: data/processed_data\\TXN\n",
      "‚úÖ Folder already fully processed, skipping: data/processed_data\\UNH\n",
      "‚úÖ Folder already fully processed, skipping: data/processed_data\\UNP\n",
      "‚úÖ Folder already fully processed, skipping: data/processed_data\\UPS\n",
      "‚úÖ Folder already fully processed, skipping: data/processed_data\\USB\n",
      "‚úÖ Folder already fully processed, skipping: data/processed_data\\V\n",
      "‚úÖ Folder already fully processed, skipping: data/processed_data\\VZ\n",
      "‚úÖ Folder already fully processed, skipping: data/processed_data\\WFC\n",
      "‚úÖ Folder already fully processed, skipping: data/processed_data\\WMT\n",
      "‚úÖ Folder already fully processed, skipping: data/processed_data\\XOM\n",
      "\n",
      "üìä Run summary:\n",
      "   - Total .md files seen: 12427\n",
      "   - Newly processed files this run: 0\n",
      "   - Total chunks created this run: 0\n",
      "   - Processed files log: vectorstores\\processed_files.txt\n",
      "   - Vector stores root: vectorstores\n",
      "‚úÖ Done.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "from typing import Set\n",
    "import time\n",
    "\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_classic.text_splitter import CharacterTextSplitter\n",
    "from langchain_classic.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# üîπ Root folders containing processed markdown data\n",
    "PROCESSED_DATA_DIRS = [\n",
    "    \"data/processed_data\",\n",
    "]\n",
    "\n",
    "GLOB_PATTERN = \"*.md\"\n",
    "\n",
    "# üîπ Root directory for FAISS store\n",
    "VECTOR_STORE_ROOT = \"vectorstores\"\n",
    "\n",
    "# üîπ File where we track which markdown files have already been processed\n",
    "PROCESSED_FILES_LOG = os.path.join(VECTOR_STORE_ROOT, \"processed_files.txt\")\n",
    "\n",
    "\n",
    "def add_documents_in_batches(vector_store, docs, batch_size: int = 64):\n",
    "    \"\"\"\n",
    "    Add documents to a FAISS store in smaller batches.\n",
    "    This is mainly for memory friendliness; no API calls here.\n",
    "    \"\"\"\n",
    "    for i in range(0, len(docs), batch_size):\n",
    "        batch = docs[i : i + batch_size]\n",
    "        print(\n",
    "            f\"   - Adding batch {i // batch_size + 1} \"\n",
    "            f\"({len(batch)} docs)...\"\n",
    "        )\n",
    "        vector_store.add_documents(batch)\n",
    "        # tiny pause just so logs are readable; not required\n",
    "        time.sleep(0.1)\n",
    "\n",
    "\n",
    "def load_processed_files(log_path: str) -> Set[str]:\n",
    "    \"\"\"Load already processed file paths from log file.\"\"\"\n",
    "    if not os.path.exists(log_path):\n",
    "        return set()\n",
    "    with open(log_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = [line.strip() for line in f if line.strip()]\n",
    "    return set(lines)\n",
    "\n",
    "\n",
    "def append_processed_files(log_path: str, file_paths):\n",
    "    \"\"\"Append newly processed file paths to log file.\"\"\"\n",
    "    os.makedirs(os.path.dirname(log_path), exist_ok=True)\n",
    "    with open(log_path, \"a\", encoding=\"utf-8\") as f:\n",
    "        for p in file_paths:\n",
    "            f.write(p + \"\\n\")\n",
    "\n",
    "\n",
    "def make_vector_dir_for_folder(folder_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Always return the same FAISS directory path\n",
    "    (single global index for all folders).\n",
    "    \"\"\"\n",
    "    return os.path.join(VECTOR_STORE_ROOT, \"index\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    os.makedirs(VECTOR_STORE_ROOT, exist_ok=True)\n",
    "\n",
    "    # üîπ Load global processed file list\n",
    "    processed_files = load_processed_files(PROCESSED_FILES_LOG)\n",
    "    print(f\"Loaded {len(processed_files)} previously processed files from log.\")\n",
    "\n",
    "    # üîπ Shared splitter & embeddings (reused for all folders)\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        # change to \"cuda\" if you have a GPU and want speed\n",
    "        model_kwargs={\"device\": \"cpu\"},\n",
    "        encode_kwargs={\"normalize_embeddings\": True},\n",
    "    )\n",
    "\n",
    "    total_md_files = 0\n",
    "    total_new_files = 0\n",
    "    total_chunks = 0\n",
    "\n",
    "    # We‚Äôll maintain a single global FAISS index\n",
    "    folder_vs_dir = make_vector_dir_for_folder(\"unused\")\n",
    "    vector_store = None\n",
    "    if os.path.exists(folder_vs_dir):\n",
    "        print(f\"\\nüì¶ Loading existing FAISS index from: {folder_vs_dir}\")\n",
    "        vector_store = FAISS.load_local(\n",
    "            folder_vs_dir,\n",
    "            embeddings,\n",
    "            allow_dangerous_deserialization=True,\n",
    "        )\n",
    "\n",
    "    for root_dir in PROCESSED_DATA_DIRS:\n",
    "        print(f\"\\nüîç Walking root: {root_dir}\")\n",
    "\n",
    "        # Walk every subfolder under this root (including the root itself)\n",
    "        for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "            # All markdown files in this specific folder\n",
    "            md_files = [\n",
    "                os.path.join(dirpath, f)\n",
    "                for f in filenames\n",
    "                if f.lower().endswith(\".md\")\n",
    "            ]\n",
    "\n",
    "            if not md_files:\n",
    "                continue\n",
    "\n",
    "            total_md_files += len(md_files)\n",
    "\n",
    "            # Filter to only files we haven't processed yet\n",
    "            new_files = [f for f in md_files if f not in processed_files]\n",
    "            if not new_files:\n",
    "                print(f\"‚úÖ Folder already fully processed, skipping: {dirpath}\")\n",
    "                continue\n",
    "\n",
    "            print(\n",
    "                f\"\\nüìÅ Folder: {dirpath}\\n\"\n",
    "                f\"   - Total .md files: {len(md_files)}\\n\"\n",
    "                f\"   - New files to process: {len(new_files)}\"\n",
    "            )\n",
    "\n",
    "            # üîπ Load only the new files\n",
    "            docs = []\n",
    "            for fpath in new_files:\n",
    "                loader = TextLoader(fpath, encoding=\"utf-8\")\n",
    "                docs.extend(loader.load())\n",
    "\n",
    "            if not docs:\n",
    "                print(\"   - No docs loaded (unexpected), skipping folder.\")\n",
    "                continue\n",
    "\n",
    "            # üîπ Chunk\n",
    "            doc_splits = text_splitter.split_documents(docs)\n",
    "            total_chunks += len(doc_splits)\n",
    "            print(f\"   - Chunks created: {len(doc_splits)}\")\n",
    "\n",
    "            # üîπ Build or update FAISS global index\n",
    "            if vector_store is None:\n",
    "                # First time: create index from first batch\n",
    "                print(f\"   - Creating new FAISS index at: {folder_vs_dir}\")\n",
    "\n",
    "                first_batch_size = min(64, len(doc_splits))\n",
    "                first_batch = doc_splits[:first_batch_size]\n",
    "                rest = doc_splits[first_batch_size:]\n",
    "\n",
    "                vector_store = FAISS.from_documents(first_batch, embeddings)\n",
    "\n",
    "                if rest:\n",
    "                    add_documents_in_batches(vector_store, rest, batch_size=64)\n",
    "\n",
    "            else:\n",
    "                print(f\"   - Updating existing FAISS index at: {folder_vs_dir}\")\n",
    "                add_documents_in_batches(vector_store, doc_splits, batch_size=64)\n",
    "\n",
    "            # üîπ Save index **after this folder**\n",
    "            os.makedirs(folder_vs_dir, exist_ok=True)\n",
    "            vector_store.save_local(folder_vs_dir)\n",
    "            print(f\"   üíæ Saved updated FAISS index to: {folder_vs_dir}\")\n",
    "\n",
    "            # üîπ Mark these files as processed so reruns skip them\n",
    "            append_processed_files(PROCESSED_FILES_LOG, new_files)\n",
    "            processed_files.update(new_files)\n",
    "            total_new_files += len(new_files)\n",
    "            print(f\"   üìù Logged {len(new_files)} processed files.\")\n",
    "\n",
    "    print(\"\\nüìä Run summary:\")\n",
    "    print(f\"   - Total .md files seen: {total_md_files}\")\n",
    "    print(f\"   - Newly processed files this run: {total_new_files}\")\n",
    "    print(f\"   - Total chunks created this run: {total_chunks}\")\n",
    "    print(f\"   - Processed files log: {PROCESSED_FILES_LOG}\")\n",
    "    print(f\"   - Vector stores root: {VECTOR_STORE_ROOT}\")\n",
    "    print(\"‚úÖ Done.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d57b186",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_21384\\3769024782.py:6: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(\n"
     ]
    }
   ],
   "source": [
    "from langchain_classic.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "VECTOR_STORE_DIR = \"vectorstores/index\"\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        # change to \"cuda\" if you have a GPU and want speed\n",
    "        model_kwargs={\"device\": \"cpu\"},\n",
    "        encode_kwargs={\"normalize_embeddings\": True},\n",
    "    )\n",
    "vector_store = FAISS.load_local(VECTOR_STORE_DIR, embeddings, allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "905af763",
   "metadata": {},
   "outputs": [],
   "source": [
    "retreiver = vector_store.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={'k': 5}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "603b5279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple Inc. | 2024 Form 10-K | 38\n",
      "\n",
      "Note 5 - Property, Plant and Equipment\n",
      "\n",
      "The following table shows the Company's gross property, plant and equipment by major asset class and accumulated depreciation as of September 28, 2024 and September 30, 2023 (in millions): {'source': 'data/processed_data\\\\AAPL\\\\10-K_2024-11-01.md'}\n",
      "Apple Inc. | 2023 Form 10-K | 38\n",
      "\n",
      "Note 5 - Property, Plant and Equipment\n",
      "\n",
      "The following table shows the Company's gross property, plant and equipment by major asset class and accumulated depreciation as of September 30, 2023 and September 24, 2022 (in millions): {'source': 'data/processed_data\\\\AAPL\\\\10-K_2023-11-03.md'}\n",
      "Apple Inc. | Q1 2024 Form 10-Q | 14\n",
      "\n",
      "Products and Services Performance The following table shows net sales by category for the three months ended December 30, 2023 and December 31, 2022 (dollars in millions): {'source': 'data/processed_data\\\\AAPL\\\\10-Q_2024-02-02.md'}\n",
      "Apple Inc. | 2025 Form 10-K | 47\n",
      "\n",
      "The following tables show net sales for 2025, 2024 and 2023 and long-lived assets as of September 27, 2025 and September 28, 2024 for countries that individually accounted for 10% or more of the respective totals, as well as aggregate amounts for the remaining countries (in millions): {'source': 'data/processed_data\\\\AAPL\\\\10-K_2025-10-31.md'}\n",
      "Depreciation expense on property, plant and equipment was $ 8.0 billion, $ 8.2 billion and $ 8.5 billion during 2025, 2024 and 2023, respectively.\n",
      "\n",
      "Apple Inc. | 2025 Form 10-K | 39\n",
      "\n",
      "Note 6 - Consolidated Financial Statement Details\n",
      "\n",
      "The following tables show the Company's consolidated financial statement details as of September 27, 2025 and September 28, 2024 (in millions):\n",
      "\n",
      "Other Non-Current Assets {'source': 'data/processed_data\\\\AAPL\\\\10-K_2025-10-31.md'}\n"
     ]
    }
   ],
   "source": [
    "query = \"Profit and losses of Apple in 2025\"\n",
    "docs = retreiver.invoke(query)\n",
    "for doc in docs:\n",
    "    print(doc.page_content, doc.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff1621e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ANSWER ===\n",
      "\n",
      "Based on the provided information, the profit and losses of Apple in 2025 cannot be directly determined. However, we can find some relevant data.\n",
      "\n",
      "From the table in Apple Inc. | 2025 Form 10-K | 47, Apple's net sales for the year ended 2025 is $434.7 billion.\n",
      "\n",
      "Depreciation expense on property, plant and equipment was $8.0 billion in 2025 as per Apple Inc. | 2025 Form 10-K | 39.\n",
      "\n",
      "To calculate the profit of Apple in 2025, we would need the operating income or net income, which is not directly provided in the context.\n",
      "\n",
      "=== SOURCES ===\n",
      "[1] data/processed_data\\AAPL\\10-K_2024-11-01.md\n",
      "[2] data/processed_data\\AAPL\\10-K_2023-11-03.md\n",
      "[3] data/processed_data\\AAPL\\10-Q_2024-02-02.md\n",
      "[4] data/processed_data\\AAPL\\10-K_2025-10-31.md\n",
      "[5] data/processed_data\\AAPL\\10-K_2025-10-31.md\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_classic.prompts import ChatPromptTemplate\n",
    "from langchain_classic.chains import RetrievalQA\n",
    "\n",
    "prompt = ChatPromptTemplate(messages=[\n",
    "        (\"system\", \"You are a precise financial research assistant. Answer concisely and cite sources by filename\"),\n",
    "        (\"human\", \"Quesion: \\n\\n {question} \\n\\n Use the following context: {context} \\n\\n Answer:\")\n",
    "    ])\n",
    "\n",
    "llm = ChatGroq(model=\"llama-3.1-8b-instant\")\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type = \"stuff\", # stuffs retrieved docs\n",
    "    retriever = retreiver,\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": prompt}\n",
    ")\n",
    "\n",
    "result = qa({\"query\": query})\n",
    "print(\"\\n=== ANSWER ===\\n\")\n",
    "print(result[\"result\"])\n",
    "\n",
    "\n",
    "print(\"\\n=== SOURCES ===\")\n",
    "for i, d in enumerate(result[\"source_documents\"], 1):\n",
    "    src = d.metadata.get(\"source\") or d.metadata.get(\"file_path\") or \"unknown\"\n",
    "    print(f\"[{i}] {src}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
